{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following method iterates through a date period then collect tweets for each day while sleeping for random seconds\n",
    "#This helps avoid being stopped by Twitter despite having an Academic Research account which allows full archive search\n",
    "from datetime import date, timedelta\n",
    "import tweepy\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "#This is needed for a full archival search\n",
    "client = tweepy.Client(bearer_token='Replace_with_your_bearer_key')\n",
    "\n",
    "#method to loop on a daily basis between two dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "#query needed to search Twitter        \n",
    "query = \"YourQuery\"\n",
    "\n",
    "#you can specify which country you want tweets from\n",
    "country = \"place_country:GB\"\n",
    "\n",
    "#this is just to simplify the call\n",
    "queryCombined = query + \" \" + country\n",
    "print(queryCombined)\n",
    "\n",
    "#your starting and ending dates (Twitter Academic allows full search back to March 2006)\n",
    "start_date = date(2018, 9, 1)\n",
    "end_date = date(2018, 9, 10)\n",
    "\n",
    "#flag to avoid printing CSV headers everytime\n",
    "printHeaders = 1\n",
    "\n",
    "#looping through dates and calling the Tweepy method needed to download tweets\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    try:\n",
    "        start_date = single_date.strftime(\"%Y-%m-%dT00:00:00Z\")\n",
    "        dtObj = datetime.datetime.strptime(start_date, \"%Y-%m-%dT00:00:00Z\")\n",
    "        end_date = dtObj + timedelta(days=1)\n",
    "        start_date_str = str(start_date).replace(\" 00\", \"T00\").replace(\"00 \", \"00Z\")\n",
    "        end_date_str = str(end_date).replace(\" 00\", \"T00\")+\"Z\"\n",
    "        print(start_date_str)\n",
    "        print(end_date_str)\n",
    "        downloadTweets(start_date_str,end_date_str,queryCombined,\"2019-2020-data.csv\",client,printHeaders)\n",
    "        printHeaders = 0\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #sleep for random seconds\n",
    "    sleep(randint(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadTweets(start_time, end_time, query, csv_file,client,printHeaders):\n",
    "    import tweepy\n",
    "\n",
    "    import tweepy\n",
    "    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "    import csv\n",
    "    from urlextract import URLExtract\n",
    "\n",
    "    extractor = URLExtract()\n",
    "\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if printHeaders>0:\n",
    "            writer.writerow([\"tweet_id\",\"tweet_text\",\"dateTime\",\"author_id\",\"retweet_count\",\"like_count\",\"quote_count\",\"reply_count\",\"UserLocation\",\"Tweet_Place\",\"URLs\", \"mentions\", \"hashtags\", \"sentiment\"])\n",
    "            file.flush()\n",
    "\n",
    "        #I'm getting the geo location of the tweet as well as the location of the user and setting the number of tweets returned to 10 (minimum) - Max is 100\n",
    "        tweets = client.search_all_tweets(query=query, tweet_fields=['context_annotations','created_at', 'public_metrics'],\n",
    "                                          place_fields=['place_type', 'geo'], user_fields=['location'], expansions='author_id,geo.place_id',\n",
    "                                          start_time=start_time,\n",
    "                                          end_time=end_time, max_results=100)\n",
    "        #in case of an empty result\n",
    "        if tweets.data is None:\n",
    "            print(\"\")\n",
    "        else:\n",
    "            print(len(tweets.data))\n",
    "\n",
    "            # Get list of places and users\n",
    "            places = {p[\"id\"]: p for p in tweets.includes['places']}\n",
    "            users = {u[\"id\"]: u for u in tweets.includes['users']}\n",
    "\n",
    "            #loop through the tweets to get the tweet ID, Text, Author ID, User Location, Tweet Location + public_metrics\n",
    "            for tweet in tweets.data:\n",
    "                print(tweet.id)\n",
    "                print(tweet.text)\n",
    "                print(tweet.created_at)\n",
    "                print(tweet.author_id)\n",
    "                print(tweet.public_metrics['retweet_count'])\n",
    "                print(tweet.public_metrics['like_count'])\n",
    "                print(tweet.public_metrics['quote_count'])\n",
    "                print(tweet.public_metrics['reply_count'])\n",
    "                userLocation = \"\"\n",
    "                placeFullName = \"\"\n",
    "                if users[tweet.author_id]:\n",
    "                    user = users[tweet.author_id]\n",
    "                    userLocation = user.location\n",
    "                if places[tweet.geo['place_id']]:\n",
    "                    place = places[tweet.geo['place_id']]\n",
    "                    placeFullName = place.full_name\n",
    "                    print(\"================\")\n",
    "                cleaned_tweet =clean_tweet(tweet.text)\n",
    "                mentions = re.findall(\"@([a-zA-Z0-9_]{1,50})\", tweet.text)\n",
    "                hashtags = re.findall(\"#([a-zA-Z0-9_]{1,50})\", tweet.text)\n",
    "                urls = extractor.find_urls(tweet.text)\n",
    "                writer.writerow([tweet.id,tweet.text,tweet.created_at,tweet.author_id,tweet.public_metrics['retweet_count'],tweet.public_metrics['like_count'],tweet.public_metrics['quote_count'],tweet.public_metrics['reply_count'],userLocation,placeFullName,urls,mentions,hashtags,get_tweet_sentiment(cleaned_tweet)])\n",
    "                file.flush()\n",
    "\n",
    "    file.close()#closing the CSV file but (the code appends to the file)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect sentiment using Text Blob\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "\n",
    "  # create TextBlob object of the passed tweet text\n",
    "    blob = TextBlob(clean_tweet(tweet))\n",
    " \n",
    "  # get sentiment\n",
    "    if blob.sentiment.polarity > 0:\n",
    "        sentiment = 'positive'\n",
    "    elif blob.sentiment.polarity < 0:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    " \n",
    "    return sentiment\n",
    "\n",
    "# testing tweet sentiment\n",
    "sample_tweet = \"Why don’t US is doing completely lockdown, @obama we are sitting home @test doing #sacrifice for #ourself & community . while many of others enjoying freedom & doing shopping’s & making other people’s infected#Coronavirus\"\n",
    "cleaned_tweet =clean_tweet(sample_tweet)\n",
    "mentions = re.findall(\"@([a-zA-Z0-9_]{1,50})\", sample_tweet)\n",
    "hashtags = re.findall(\"#([a-zA-Z0-9_]{1,50})\", sample_tweet)\n",
    "print(cleaned_tweet)\n",
    "print(get_tweet_sentiment(sample_tweet)) # Output: negative\n",
    "print(mentions)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean Tweets\n",
    "\n",
    "import re # importing regex\n",
    "import string\n",
    " \n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Remove unncessary things from the tweet \n",
    "    like mentions, hashtags, URL links, punctuations\n",
    "    '''\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    # remove mentions\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet)  \n",
    "\n",
    "    # remove punctuations like quote, exclamation sign, etc.\n",
    "    # we replace them with a space\n",
    "    tweet = re.sub(r'['+string.punctuation+']+', ' ', tweet)\n",
    "\n",
    "    return tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to extract URLs from text\n",
    "from urlextract import URLExtract\n",
    "\n",
    "extractor = URLExtract()\n",
    "urls = extractor.find_urls(\"This is a link http://www.google.com another link www.yahoo.com, and another htts://www.anotherlink.com and another http://hello.org\")\n",
    "print(urls) # prints: ['stackoverflow.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
